{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFFkxSWTv0xI"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender\n",
        "!pip install imageio==2.4.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "te_o5LjQv3NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from colabgymrender.recorder import Recorder"
      ],
      "metadata": {
        "id": "cyfN4LEvv4eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "directory = './video'\n",
        "env = Recorder(env, directory)"
      ],
      "metadata": {
        "id": "wMhrbRqewmUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_STATES = env.observation_space.n\n",
        "NUM_ACTIONS = env.action_space.n"
      ],
      "metadata": {
        "id": "21TpKDICwpYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_table = np.random.rand(NUM_STATES, NUM_ACTIONS)"
      ],
      "metadata": {
        "id": "Ublcl2lWwppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0: LEFT\n",
        "\n",
        "1: DOWN\n",
        "\n",
        "2: RIGHT\n",
        "\n",
        "3: UP"
      ],
      "metadata": {
        "id": "g586iPlLwtaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q_table[0, 2] = 1\n",
        "Q_table[1, 2] = 1\n",
        "Q_table[2, 1] = 1\n",
        "Q_table[6, 1] = 1\n",
        "Q_table[10,1] = 1\n",
        "Q_table[14,2] = 1"
      ],
      "metadata": {
        "id": "RYcqvysswydO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "terminal = False\n",
        "\n",
        "while not terminal:\n",
        "  action = np.argmax(Q[observation,:])\n",
        "  observation, reward, done, info = env.step(action)\n",
        "\n",
        "print(f'Final reward = {reward}')\n",
        "\n",
        "env.play()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "6VmSzm0Bw0sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/Pong-v5\")\n",
        "directory = './video'\n",
        "env = Recorder(env, directory)"
      ],
      "metadata": {
        "id": "4GnwsGJ5v9Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(s):\n",
        "    action = env.action_space.sample() # случайная стратегия\n",
        "    return action"
      ],
      "metadata": {
        "id": "ZPdjwUFJwZ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "totalReward = 0\n",
        "\n",
        "for _ in range(1000):\n",
        "    action = policy(obs) # случайная стратегия\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    totalReward += reward\n",
        "    if reward != 0:\n",
        "        print('New reward = {}'.format(reward))\n",
        "    if done:        \n",
        "        break\n",
        "\n",
        "env.play()      \n",
        "env.close()\n",
        "\n",
        "print('Total reward = {}'.format(totalReward))"
      ],
      "metadata": {
        "id": "Rcf37z57wBFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "ux0qeAT4xFpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_STATES = env.observation_space.n # количество состояний\n",
        "NUM_ACTIONS = env.action_space.n # количество действий\n",
        "\n",
        "lr = 0.3 # learning rate\n",
        "gamma = 0.6 # параметр дисконтирования\n",
        "\n",
        "NUM_EPISODES = 1500 # число эпизодов для обучения"
      ],
      "metadata": {
        "id": "GlFC2-X1xsAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathLenList = [] # длины траекторий по эпизодам\n",
        "totalRewardList = [] # суммарные награды по эпизодам\n",
        "\n",
        "Q = np.random.rand(NUM_STATES, NUM_ACTIONS) # Инициализация Q-функции (таблицы)\n",
        "\n",
        "for i in range(1, NUM_EPISODES+1):\n",
        "    \n",
        "    s = env.reset()\n",
        "\n",
        "    totalReward = 0\n",
        "    step = 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "\n",
        "        a = np.argmax(Q[s,:]) # Выбор действия по текущей политике\n",
        "        s1, r, done, _ = env.step(a) # Сделать шаг\n",
        "        \n",
        "        # Новое (целевое) значение Q-функции\n",
        "        if done:\n",
        "            Q_target = r\n",
        "        else:\n",
        "            Q_target = r + gamma * np.max(Q[s1,:])             \n",
        "        Q[s,a] = (1-lr) * Q[s,a] + lr * Q_target # Обновление Q-функции\n",
        "        \n",
        "        totalReward += r\n",
        "        s = s1\n",
        "            \n",
        "    pathLenList.append(step)\n",
        "    totalRewardList.append(totalReward)\n",
        "    if i % 100 == 0:\n",
        "      print('Episode {}: Total reward = {}'.format(i, totalReward))     "
      ],
      "metadata": {
        "id": "_zj39tX-xvEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(pathLenList)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "SBUqmB_Gxx35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(totalRewardList)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "6LQxB5hzxy_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "\n",
        "NUM_STATES = env.observation_space.n\n",
        "NUM_ACTIONS = env.action_space.n"
      ],
      "metadata": {
        "id": "swYCdZID_ETQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.8 # learning rate\n",
        "gamma = 0.95 # параметр дисконтирования\n",
        "\n",
        "NUM_EPISODES = 50 # число эпизодов для обучения\n",
        "MAX_STEPS = 100 # максимальное число шагов в эпизоде"
      ],
      "metadata": {
        "id": "2uEoZm40_GHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pathLenList = [] # длины траекторий по эпизодам\n",
        "totalRewardList = [] # суммарные награды по эпизодам\n",
        "\n",
        "# Инициализация Q-функции (таблицы)\n",
        "Q = np.random.rand(NUM_STATES, NUM_ACTIONS)\n",
        "\n",
        "for i in range(NUM_EPISODES):\n",
        "    \n",
        "    s = env.reset()\n",
        "\n",
        "    totalReward = 0\n",
        "    step = 0\n",
        "\n",
        "    while step < MAX_STEPS:\n",
        "        step += 1\n",
        "            \n",
        "        # Выбор действия по текущей политике\n",
        "        a = np.argmax(Q[s,:])\n",
        "        \n",
        "        # Сделать шаг\n",
        "        s1, r, done, _ = env.step(a)\n",
        "        \n",
        "        # Новое (целевое) значение Q-функции\n",
        "        if done:\n",
        "            Q_target = r\n",
        "        else:\n",
        "            Q_target = r + gamma * np.max(Q[s1,:])\n",
        "            \n",
        "        # Обновление Q-функции\n",
        "        Q[s,a] = (1-lr) * Q[s,a] + lr * Q_target\n",
        "        \n",
        "        totalReward += r\n",
        "        s = s1\n",
        "        \n",
        "        # Если конец эпизода\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    pathLenList.append(step)\n",
        "    totalRewardList.append(totalReward)\n",
        "    print('Episode {}: Total reward = {}'.format(i, totalReward))  "
      ],
      "metadata": {
        "id": "2xqjhOec_H_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  a = np.argmax(Q[s,:])\n",
        "  s, r, done, _ = env.step(a)\n",
        "\n",
        "print(f'Final reward = {r}')\n",
        "\n",
        "env.play()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "qy1CCTDE_J95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "7sLGJZ5a_fS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "directory = './video'\n",
        "env = Recorder(env, directory)"
      ],
      "metadata": {
        "id": "gXOKFnHC_rv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1 # learning rate\n",
        "gamma = 0.99 # параметр дисконтирования\n",
        "\n",
        "NUM_STATES = env.observation_space.n\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "\n",
        "NUM_EPISODES = 4000 # число эпизодов для обучения\n",
        "MAX_STEPS = 300\n",
        "\n",
        "batch_size=32"
      ],
      "metadata": {
        "id": "KFoOGHZi_tZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.memory = deque(maxlen=2500)\n",
        "        self.learning_rate=0.001\n",
        "        self.epsilon=1\n",
        "        self.max_eps=1\n",
        "        self.min_eps=0.01\n",
        "        self.eps_decay = 0.001/3\n",
        "        self.gamma=0.9\n",
        "        self.state_size= state_size\n",
        "        self.action_size= action_size\n",
        "        self.epsilon_lst=[]\n",
        "        self.model = self.buildmodel()\n",
        "\n",
        "    def buildmodel(self):\n",
        "        model=Sequential()\n",
        "        model.add(Dense(10, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def add_memory(self, new_state, reward, done, state, action):\n",
        "        self.memory.append((new_state, reward, done, state, action))\n",
        "\n",
        "    def action(self, state):\n",
        "        if np.random.rand() > self.epsilon:\n",
        "            return np.random.randint(0,4)\n",
        "        return np.argmax(self.model.predict(state))\n",
        "\n",
        "    def pred(self, state):\n",
        "        return np.argmax(self.model(state))\n",
        "\n",
        "    def replay(self,batch_size):\n",
        "        minibatch=random.sample(self.memory, batch_size)\n",
        "        for new_state, reward, done, state, action in minibatch:\n",
        "            target= reward\n",
        "            if not done:\n",
        "                target=reward + self.gamma* np.amax(self.model(new_state))\n",
        "            target_f= self.model(state)\n",
        "            target_f[0][action]= target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.min_eps:\n",
        "            self.epsilon=(self.max_eps - self.min_eps) * np.exp(-self.eps_decay*episode) + self.min_eps\n",
        "\n",
        "        self.epsilon_lst.append(self.epsilon)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "agent=Agent(NUM_STATES, NUM_ACTIONS)"
      ],
      "metadata": {
        "id": "05WbO-YK_vam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_lst=[]\n",
        "for episode in range(NUM_EPISODES):\n",
        "    state= env.reset()\n",
        "    state_arr=np.zeros(NUM_STATES)\n",
        "    state_arr[state] = 1\n",
        "    state= np.reshape(state_arr, [1, NUM_STATES])\n",
        "    reward = 0\n",
        "    done = False\n",
        "    t = 0\n",
        "    for t in range(MAX_STEPS):\n",
        "        # env.render()\n",
        "        #t += 1\n",
        "        action = agent.action(state)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_arr = np.zeros(NUM_STATES)\n",
        "        new_state_arr[new_state] = 1\n",
        "        new_state = np.reshape(new_state_arr, [1, NUM_STATES])\n",
        "        agent.add_memory(new_state, reward, done, state, action)\n",
        "        state= new_state\n",
        "\n",
        "        if done:  \n",
        "            break\n",
        "\n",
        "    reward_lst.append(reward)\n",
        "\n",
        "    if len(agent.memory)> batch_size:\n",
        "        print(f'Episode: {episode:4}/{NUM_EPISODES} and step: {t:4}. Eps: {float(agent.epsilon):.2}, reward {reward}')\n",
        "        agent.replay(batch_size)\n",
        "\n",
        "print(' Train mean % score= ', round(100*np.mean(reward_lst),1))"
      ],
      "metadata": {
        "id": "ZVt-0AKi_0I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.save(name='DQN_FrozenLake-v1_4x4')"
      ],
      "metadata": {
        "id": "EevyKOhm_3EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = env.reset()\n",
        "s_arr=np.zeros(NUM_STATES)\n",
        "s_arr[s] = 1\n",
        "s= np.reshape(s_arr, [1, NUM_STATES])\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  a = agent.pred(s)\n",
        "  s1, r, done, _ = env.step(a)\n",
        "  s1_arr = np.zeros(NUM_STATES)\n",
        "  s1_arr[s1] = 1\n",
        "  s1 = np.reshape(s1_arr, [1, NUM_STATES])\n",
        "  s = s1\n",
        "\n",
        "print(f'Final reward = {r}')\n",
        "\n",
        "env.play()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "4ygiGAiP_5Mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}